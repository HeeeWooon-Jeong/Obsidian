
# GRU
```python
import torch

from torch.nn import GRU

from torch import nn

from sklearn.model_selection import train_test_split

import torch.optim as optim

from torch.utils.data import DataLoader, TensorDataset

from sklearn.model_selection import StratifiedKFold

  
  

# GRU 모델 정의

class GRUModel(nn.Module):

    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):

        super(GRUModel, self).__init__()

        self.gru = GRU(input_dim, hidden_dim, num_layers, batch_first=True)

        self.fc = nn.Linear(hidden_dim, output_dim)

  

    def forward(self, x):

        out, _ = self.gru(x)

        out = self.fc(out[:, -1, :])

        return out

  

device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # GPU 지원 가능 확인

  

# Initialize the GRU model

model = GRUModel(input_dim=9, hidden_dim=256, num_layers=3, output_dim=4).to(device)

  

# 손실 함수 및 옵티마이저 정의

criterion = nn.CrossEntropyLoss()  # Categorical Cross-Entropy 손실

# criterion = nn.BCELoss()  # Binary Cross-Entropy Loss

optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam 옵티마이저

# lr=0.001은 학습률을 0.001로 설정한 것입니다. 이 값은 일반적으로 사용되는 학습률 중 하나이며, 신경망 모델을 효과적으로 학습시키는 데 사용됩니다

# eta?

  
  

# Prepare data

x = dftest[['len', 'letters_count', 'digits_count', 'special_chars_count','shortened','abnormal_url', 'have_ip', 'url_region', 'root_domain']].values

y = dftest['url_type'].values

  

# Split the data

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=88)

  

# Convert to tensor

x_train = torch.tensor(x_train, dtype=torch.float32).unsqueeze(-2)

y_train = torch.tensor(y_train, dtype=torch.long)

x_test = torch.tensor(x_test, dtype=torch.float32).unsqueeze(-2)

y_test = torch.tensor(y_test, dtype=torch.long)

  

# Create dataloaders

train_data = TensorDataset(x_train, y_train)

test_data = TensorDataset(x_test, y_test)

  

train_loader = DataLoader(train_data, batch_size=32, shuffle=True)

test_loader = DataLoader(test_data, batch_size=32)

  
  

correct = 0

total = 0

  

best_model_path = '/content/drive/MyDrive/10_25모델/GRU_10_25-2.pth' #모델 저장할 경로

best_accuracy = 0.0  # 최고 정확도를 추적할 변수 초기화

epochs= 159

  

# Training loop

for epoch in range(epochs):  # Define the number of epochs

    total_loss = 0

  

    for i, (inputs, labels) in enumerate(train_loader):

        inputs, labels = inputs.to(device), labels.to(device)

  

        # Zero the parameter gradients

        optimizer.zero_grad()

  

        # Forward + backward + optimize

        outputs = model(inputs)

        loss = criterion(outputs, labels)

        loss.backward()

        optimizer.step()

  

        total_loss += loss.item()

        # 정확도 계산

        _, predicted = torch.max(outputs, 1)

        total += labels.size(0)

        correct += (predicted == labels).sum().item()

        # 매 스텝마다 정확도 출력

        step_accuracy = 100 * correct / total

        print(f'에포크 [{epoch + 1}/{epochs}], 스텝 [{i + 1}/{len(train_loader)}], 손실: {loss.item():.4f}, 정확도: {step_accuracy:.2f}%')

    average_loss = total_loss / len(train_loader)

    accuracy = 100 * correct / total  # 정확도 계산

    print(f'에포크 [{epoch + 1}/{epochs}], 평균 손실: {average_loss:.4f}, 정확도: {accuracy:.2f}%')

  
  

# 현재 모델의 검증 정확도를 계산 (검증 데이터를 사용하여 계산)

model.eval()

with torch.no_grad():

    val_correct = 0

    val_total = 0

    for val_inputs, val_labels in test_loader:

        val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)

        val_outputs = model(val_inputs)

        _, val_predicted = torch.max(val_outputs, 1)

        val_total += val_labels.size(0)

        val_correct += (val_predicted == val_labels).sum().item()

    val_accuracy = 100 * val_correct / val_total

    print(f'검증 정확도: {val_accuracy:.2f}%')

model.train()

  

# 만약 현재 검증 정확도가 최고 정확도보다 높으면 모델을 저장

if val_accuracy > best_accuracy:

    best_accuracy = val_accuracy

    torch.save(model.state_dict(), best_model_path)

    print('모델 저장: 검증 정확도가 향상되었습니다.')

  

print('훈련 완료')

```

# 과적합 방지 
```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import StratifiedKFold
# GRU 모델 정의
class GRUModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):
        super(GRUModel, self).__init__()
        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        out, _ = self.gru(x)
        out = self.fc(out[:, -1, :]) 
        return out

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 데이터 준비
x = dftest[['len', 'letters_count', 'digits_count', 'special_chars_count','shortened','abnormal_url', 'have_ip', 'url_region', 'root_domain']].values
y = dftest['url_type'].values

# 모델 및 학습 관련 설정
input_dim = x.shape[1]
hidden_dim = 256
num_layers = 3
output_dim = 4
lr = 0.001
epochs = 30
batch_size = 32
k_folds = 5  # k-fold 교차 검증 횟수

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=lr)

# k-fold 교차 검증을 위한 준비
skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=88)

for fold, (train_indices, val_indices) in enumerate(skf.split(x, y)):
    x_train, x_val = x[train_indices], x[val_indices]
    y_train, y_val = y[train_indices], y[val_indices]

    x_train = torch.tensor(x_train, dtype=torch.float32)
    y_train = torch.tensor(y_train, dtype=torch.long)
    x_val = torch.tensor(x_val, dtype=torch.float32)
    y_val = torch.tensor(y_val, dtype=torch.long)

    train_data = TensorDataset(x_train, y_train)
    val_data = TensorDataset(x_val, y_val)

    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_data, batch_size=batch_size)

    model = GRUModel(input_dim, hidden_dim, num_layers, output_dim).to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr)

    best_val_accuracy = 0.0

    for epoch in range(epochs):
        # 훈련 및 검증 루프 내부 로직은 이전 코드와 동일합니다.
        # 훈련 후, 각 fold의 검증 정확도를 계산하고 가장 높은 정확도를 가진 모델을 저장합니다.

        if val_accuracy > best_val_accuracy:
            best_val_accuracy = val_accuracy
            torch.save(model.state_dict(), f'best_model_fold_{fold}.pth')

print('k-fold 교차 검증 완료')


```

## 하이퍼 파라미터
```python
from torch.optim.lr_scheduler import StepLR

input_dim = x.shape[1] hidden_dim = 128 # 은닉 차원 설정 
num_layers = 2 # GRU 레이어 수 설정 
output_dim = 4 lr = 0.001 # 학습률 설정 
epochs = 50 # 에포크 수 설정 
batch_size = 64 # 배치 크기 설정 
k_folds = 5 # k-fold 교차 검증 횟수
# 옵티마이저 하이퍼파라미터 설정
optimizer_params = { 'lr': 0.001, # 학습률 'betas': (0.9, 0.999), # Adam 베타 파라미터 'eps': 1e-8 # 엡실론 }
optimizer = optim.Adam(model.parameters(), **optimizer_params)

# 스케줄러 설정 (옵션)
scheduler = StepLR(optimizer, step_size=30, gamma=0.1)


					
```

# 특성 중요도 확인

```python

```