
> [!NOTE] tokenizer
> `tokenizer.fit_on_texts(X)`는 Tokenizer를 사용하여 텍스트 데이터인 `X`를 학습하고 단어 사전을 구축하는 과정을 나타냅니다. 이 과정은 텍스트 데이터를 숫자로 변환하기 위해 필요한 단골 사전을 작성합니다. 아래는 이 과정의 단계를 설명합니다:

1. **토큰화(Tokenization)**: `X`에 포함된 텍스트 데이터를 단어로 나누고 각 단어를 토큰으로 처리합니다. 일반적으로 공백이나 구두점을 기준으로 단어를 분리합니다.

2. **단어 사전 구축**: Tokenizer는 토큰화된 단어들을 기반으로 단어 사전을 구축합니다. 이 단어 사전에는 각 단어에 대한 고유한 정수 인덱스가 할당됩니다. 단어 사전은 Tokenizer 객체의 `word_index` 속성에 저장됩니다.

예를 들어, 다음과 같은 텍스트 데이터가 있다고 가정하겠습니다:

```python
X = ["This is a sample sentence.", "Another example sentence."]
```

`fit_on_texts(X)`를 호출하면 Tokenizer는 이러한 단어를 인식하고 단어 사전을 생성합니다. 결과적으로, 단어 사전은 다음과 유사한 형태가 됩니다:

```python
{
    'this': 1,
    'is': 2,
    'a': 3,
    'sample': 4,
    'sentence': 5,
    'another': 6,
    'example': 7
}
```

각 단어에 대응하는 고유한 정수 인덱스가 할당되었습니다. 이제 모델에 텍스트 데이터를 주입하기 위해 Tokenizer를 사용하면, 각 단어가 이러한 인덱스로 변환되고 시퀀스로 표현됩니다. 이를 통해 모델은 텍스트 데이터를 숫자로 처리할 수 있습니다.
