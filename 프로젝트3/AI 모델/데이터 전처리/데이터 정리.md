data_cleaning_suggestions(df) - 사용하여 받은 제안
이 제안은 높은 카디널리티(고유 값의 수)를 가지는 열(피처)을 다룰 때 유용한 방법을 제시하고 있습니다. 높은 카디널리티를 가지는 열은 고유 값의 수가 많아서 일반적인 인코딩 기법을 사용하면 데이터 차원이 커지는 문제가 발생할 수 있습니다. 이때 두 가지 대안적인 접근 방법이 있습니다: 해시 인코딩(hash encoding)과 임베딩(embedding).

1. **해시 인코딩 (Hash Encoding)**:
   - 해시 인코딩은 높은 카디널리티 열을 상대적으로 저차원의 고정 크기 해시로 매핑하는 방법입니다.
   - 이러한 해시 매핑은 고유 값들에 대한 해시 함수를 사용하여 수행됩니다. 이를 통해 원래 열을 고유 값 대신 해시 값으로 대체할 수 있습니다.
   - 주로 범주형 열에 적용되며, 다양한 머신 러닝 모델에서 고유 값의 개수가 너무 많아서 처리하기 어려울 때 유용합니다.

2. **임베딩 (Embedding)**:
   - 임베딩은 주로 텍스트 또는 범주형 데이터를 다룰 때 사용되며, 고유 값의 차원을 저차원으로 축소시키는 방법입니다.
   - 임베딩은 주로 신경망 모델과 관련이 있으며, 단어, 범주 또는 범주형 변수를 실수 벡터 공간으로 매핑합니다.
   - 텍스트 데이터를 처리할 때 워드 임베딩(Word Embedding)이 흔히 사용됩니다. 예를 들어, Word2Vec, GloVe와 같은 방법을 사용하여 단어를 저차원 벡터로 표현합니다.

높은 카디널리티를 가지는 열을 다루는 데 이러한 방법을 사용하면, 데이터의 차원을 줄이고 모델의 효율성을 향상시킬 수 있습니다. 따라서 데이터 세트에서 높은 카디널리티 열을 식별하고, 해당 열을 해시 인코딩 또는 임베딩하여 데이터의 차원을 효과적으로 관리할 수 있습니다. 이는 특히 머신 러닝 모델에 대한 입력 데이터를 준비할 때 유용합니다.




![[Pasted image 20231011162902.png]]
데이터 삭제함

![[Pasted image 20231011172407.png]]

텍스트 데이터로써 범주형 데이터에 속함
텍스트 데이터를 벡터화하고 자연어 처리(NLP) 모델을 사용해야함



[데이터셋1](https://www.kaggle.com/datasets/sid321axn/malicious-urls-dataset)
>양성, 피싱, 악성 코드 및 변조 URL을 수집하기 위해 우리는 [URL 데이터 세트(ISCX-URL-2016)를 사용했습니다. 피싱 및 악성 코드 URL을 늘리기 위해 우리는](https://www.unb.ca/cic/datasets/url-2016.html) [악성 도메인 블랙리스트 데이터 세트를](http://www.malwaredomains.com/wordpress/?page_id=66) 사용했습니다 . [faizan git repo를](https://github.com/faizann24/Using-machine-learning-to-detect-malicious-URLs/tree/master/data) 사용하여 무해한 URL을 늘렸습니다 . 마침내 [Phishtank 데이터세트](https://www.phishtank.com/developer_info.php) 와 [PhishStorm 데이터세트를](https://research.aalto.fi/en/datasets/phishstorm--phishing--legitimate-url-dataset(f49465b2-c68a-4182-9171-075f0ed797d5).html) 사용하여 피싱 URL의 수를 더 늘렸습니다 . 앞서 말씀드린 대로 데이터세트는 다양한 소스에서 수집됩니다. 그래서 먼저, 우리는 다양한 소스의 URL을 별도의 데이터 프레임으로 수집하고 마지막으로 이를 병합하여 URL과 해당 클래스 유형만 유지했습니다.

[데이터셋2](https://www.kaggle.com/datasets/shawon10/url-classification-dataset-dmoz)

---




url 과 type 컬럼 두개로 구성되어 있으며
문제데이터가 될 url에는 아랍어같은 문자가 포함되어있어 
```python
encoding='ISO-8859-1
```
로 해주어야 함

정답데이터가 될 type 컬럼에는 답이 4개이므로 
[label encoding]   or   [원-핫 인코딩]을 진행해야하고 다중분류모델이 필수적

[[tokenizer]]로 단어 사전 구축
10/13 - 토크나이저 진행시 정확한 판단 어려움(단어의 빈도수로 판단)
#### 소스코드
```python
import pandas as pd

import numpy as np

from sklearn.model_selection import train_test_split

from sklearn.preprocessing import LabelEncoder

import tensorflow as tf

from tensorflow import keras

from tensorflow.keras.layers import Embedding, LSTM, Dense

from tensorflow.keras.preprocessing.text import Tokenizer

from tensorflow.keras.preprocessing.sequence import pad_sequences

  

# 가상의 데이터 프레임 (실제 데이터로 대체해야 함)

data = pd.read_csv('./drive/MyDrive/딥러닝/data/malicious_phish.csv', encoding='ISO-8859-1')

  

# 데이터 전처리

X = data['url']

y = data['type']

  

# 레이블 인코딩

# 정답데이터의 타입이 4개 이므로 레이블 인코딩으로 모델이 학습하기 좋게 숫자로 변경

label_encoder = LabelEncoder()

y = label_encoder.fit_transform(y)

  

# tokenizer 사용하여 텍스트 데이터를 시퀀스로 변환

# 일반적으로 공백이나 구두점을 기준으로 단어를 분리

tokenizer = Tokenizer(num_words=10000)  # 등장 빈도수가 높은 상위 10,000개의 단어만 사용하여 학습 진행

tokenizer.fit_on_texts(X) # X에 대한 단어사전 구축

X = tokenizer.texts_to_sequences(X) #

X = pad_sequences(X, maxlen=100)  # 시퀀스 길이를 100으로 맞춤

  

# 데이터 분리 (훈련 데이터와 테스트 데이터)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

  

# 모델 정의

model = keras.Sequential([

    Embedding(input_dim=10000, output_dim=32, input_length=100),

    LSTM(64),

    Dense(4, activation='softmax')

])

  

# 모델 컴파일

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

  

# 모델 훈련

num_epochs = 10

batch_size = 32

  

model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size, verbose=1)

# 12:00 시작

# 모델 평가

test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)

print(f'Test accuracy: {test_accuracy * 100:.2f}%')

  

# 새로운 URL 주소

new_url = ["https123231://www.navefr.com/231vdsovjsdhv/12391823-91823-9182-398"]  # 여기에 테스트하려는 URL 주소를 추가하세요

  

# 텍스트 데이터를 시퀀스로 변환

new_url_sequences = tokenizer.texts_to_sequences(new_url)

new_url_sequences = pad_sequences(new_url_sequences, maxlen=100)  # 시퀀스 길이를 모델의 입력 길이와 일치시킵니다.

  

# 모델을 사용하여 예측

predictions = model.predict(new_url_sequences)

  

# 예측 결과를 레이블로 디코딩

predicted_labels = label_encoder.inverse_transform(np.argmax(predictions, axis=1))

  

# 예측된 레이블 출력

print("Predicted Labels:", predicted_labels)
```

---







---

