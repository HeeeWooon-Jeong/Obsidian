
# 코드 구현
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from statsmodels.stats.outliers_influence import variance_inflation_factor
from autoviz import data_cleaning_suggestions #  전처리 할때 어떻게 하면 좋을지 추천까지 해줌
from pycaret import regression # 회귀 수행 입포트 PyCaret는 파이썬 머신 러닝 라이브러리로, 머신 러닝 작업을 간소화하고 빠르게 수행할 수 있도록 도와주는 도구
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from transformers import BertModel, BertTokenizer # bert모델 bert 토크나이저
import torch # 토치 사용
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE

df = pd.read_csv('/content/drive/MyDrive/딥러닝/data/malicious_phish.csv', encoding='ISO-8859-1' )
encoded_data = le.fit_transform(df['type'])
df['type'] = encoded_data

  

model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True) # 사전 학습된 bert 가져옴
# bert-base-uncased 는 영어 텍스트 처리에 효과적 uncased는 대소문자 구별 없음
# bert-base-cased 로 시도해볼것
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # 사용하면 사전 훈련된 BERT 모델의 토크나이저를 손쉽게 가져와서 다양한 자연어 처리 작업에 사용할 수 있음
#각 트랜잭션의 특징을 추출하는 함수를 정의합니다.
def extract_features(text):
    # 토큰화 텍스트
    input_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)])
    # 토큰의 숨겨진 상태를 가져옴
    with torch.no_grad():
        outputs = model(input_ids)
        hidden_states = outputs[2]
    # 마지막 4개의 숨겨진 상태를 연결합니다.
    token_vecs = []
    for layer in range(-4, 0):
        token_vecs.append(hidden_states[layer][0])
    # 마지막 4개의 숨겨진 상태의 평균을 계산합니다.
    features = []
    for token in token_vecs:
        features.append(torch.mean(token, dim=0))

    # 기능을 텐서로 반환
    return torch.stack(features)
# 각 거래에 대한 특징 추출
features = []
for i in range(len(df)):
    features.append(extract_features(df.iloc[i]["url"])) #1시간
# 특징을 연결하고 numpy 배열로 변환합니다.
features = torch.cat(features).numpy()
types = df['type'].values
# 기능은 4000x768 크기의 2D numpy 배열입니다.
# 레이블은 크기 1000의 1D numpy 배열입니다.
# 특징 배열의 모양을 1000x(768*4) 크기로 변경합니다.
features_reshaped = features.reshape((1000, -1))
# 특징 배열을 레이블 배열과 수평으로 연결합니다.
dataset = np.hstack((features_reshaped, types.reshape((-1, 1))))
# 데이터세트는 1000x(4000*768+1) 크기의 2D numpy 배열입니다.

train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=100)
# 훈련 세트와 테스트 세트를 다시 별도의 특성 및 레이블 배열로 변환합니다.
X_train, y_train = train_data[:, :-1], train_data[:, -1]
X_test, y_test = test_data[:, :-1], test_data[:, -1]
# SMOTE 인스턴스화
sm = SMOTE(random_state=42)
# 훈련 데이터에 SMOTE를 맞춥니다
X_train_res, y_train_res = sm.fit_resample(X_train, y_train)
y_train_s = pd.Series(y_train)
y_train_res = pd.Series (y_train_res)
# 원본 데이터와 리샘플링된 데이터의 클래스 분포를 인쇄합니다.
print('Class distribution before resampling:', y_train_s.value_counts())
print('Class distribution after resampling:', y_train_res.value_counts())
from sklearn.linear_model import LogisticRegression
# 훈련 세트에 대해 로지스틱 회귀 분류기를 훈련시킵니다.
clf = LogisticRegression()
clf.fit(X_train, y_train)
# 테스트 세트에서 분류기를 평가합니다.
score = clf.score(X_test, y_test)
print("Accuracy:", score)
from sklearn.metrics import confusion_matrix, classification_report

# 혼동 행렬 및 분류 보고서 생성
cm = confusion_matrix(y_test, y_pred)
cr = classification_report(y_test, y_pred)

print("Confusion Matrix:\n", cm)
print("\nClassification Report:\n", cr)


```




# 함수 설명
```python
data_cleaning_suggestions(df)
#함수는 데이터 프레임(df)에 대한 데이터 클리닝 제안을 제공하는 함수입니다. 이 함수를 호출하면 데이터 프레임에 대한 다양한 데이터 클리닝 제안이 반환될 것입니다. 이러한 제안은 데이터 정제 및 전처리 단계에서 도움이 될 수 있습니다.

# 데이터에 결측값(missing values)이나 이상치(outliers)가 있는지 확인하고, 이러한 이슈들을 해결하는 방법에 대한 제안을 얻을 수 있습니다. 또한 데이터의 분포, 중복 레코드, 데이터 유형 등과 관련된 다양한 데이터 품질 관련 제안을 얻을 수 있습니다.
```

# 설치 라이브러리 설명
```
import pandas as pd   
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from statsmodels.stats.outliers_influence import variance_inflation_factor
from autoviz.classify_method import data_cleaning_suggestions ,data_suggestions
from pycaret import regression
from sklearn.model_selection import cross_val_score

```

---

# 라이브러리 설치 해석
```python
# `seaborn`은 Python에서 사용하는 데이터 시각화 라이브러리입니다. `matplotlib`을 기반으로 하여 보다 편리하고, 아름답게 그래프를 그릴 수 있도록 설계되었습니다.

# PyCaret은 Python에서 사용할 수 있는 오픈 소스 기계 학습 및 데이터 분석 라이브러리입니다.

# scipy import stats 통계ㅅ함수와 도구에 중점

from statsmodels.stats.outliers_influence import variance_inflation_factor
#  이 함수는 다중 회귀 분석에서 다중공선성을 평가하는 데 사용되는 Variance Inflation Factor (VIF)를 계산하는 데 활용

from autoviz.classify_method import data_cleaning_suggestions ,data_suggestions
# `autoviz` 라이브러리의 `classify_method` 모듈에서 `data_cleaning_suggestions`와 `data_suggestions` 함수를 가져오는 것입니다.
# `data_cleaning_suggestions`: 이 함수는 데이터 클리닝과 관련된 제안을 제공합니다. 이는 주로 데이터에서 결측값 처리, 이상치 처리, 데이터 정제 및 전처리와 같은 데이터 정리 작업에 관한 제안을 포함합니다.
#  `data_suggestions`: 이 함수는 데이터 분석과 관련된 제안을 제공합니다. 데이터의 특성에 따라 어떤 통계 분석, 시각화 방법, 또는 머신 러닝 모델을 사용할 것을 제안합니다.


```



# 인스톨 
```
> pip install -q autoviz
AutoViz"는 데이터를 시각화하고 탐색하는 과정을 자동화하는 Python 패키지입니다. 이 패키지를 사용하면 데이터 집합을 더 잘 이해하기 위한 다양한 시각화를 자동으로 생성할 수 있습니다.



> pip install -q -U --pre pycaret
명령은 Python 환경에서 "PyCaret" 라이브러리를 설치하거나 업그레이드하는 명령입니다. "PyCaret"는 머신 러닝 모델을 구축하고 배포하는 과정을 단순화하는 고수준, 로우코드 머신 러닝 라이브러리로, 데이터와 예측 모델을 다루는 과정을 간단하고 빠르게 만들어줍니다
```

>  '-q' quit 플래그  설치 프로세스 출력 메시지를 간소화 오류만 출력
>  '-u'  upgrade약자 이미 설치된 패키지가 있다면 최신 버전으로 업그레이드
>  '--pre'  아직 정식 릴리즈 되지 않은 사전 릴리즈 버전도 설치 대상에 포함












---

# 구상

> [!NOTE] 모델
> 텍스트 데이터에 대한 다중 분류 딥러닝 모델을 만들려면 텍스트 데이터를 벡터화하고 자연어 처리(NLP) 모델을 사용해야 합니다.

지도학습(Supervised Learning) 을 통해
"안전한 URL"과 "위험한 URL"에 대한 데이터셋
모델에 상용할 특성은 URL의 길이 도메인이름 경로의 구조 쿼리 파라미터등으로 나눈다

자연어 처리(NLP) 기법을 사용하여 URL 내의 텍스트 정보를 분석









--- 

### hdf5 형태로 최적의 모델 저장 

```python
from tensorflow import keras
from tensorflow.keras.callbacks import ModelCheckpoint

# 모델 생성 및 컴파일
model = keras.Sequential([
    keras.layers.Dense(64, activation='relu', input_shape=(784,)),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# 모델 체크포인트 콜백 설정
checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss', mode='min')

# 모델 훈련
model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), callbacks=[checkpoint])

```


# 베스트 모델을 불러와서 실행
from tensorflow.keras.models import load_model

best_model = load_model("./폴더명/파일명.hdf5")

best_model.evaluate(X, y)

---
