```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# 가상의 데이터 프레임 (실제 데이터로 대체해야 함)
data = pd.read_csv("your_data.csv")

# 데이터 전처리
X = data['url']
y = data['type']

# 레이블 인코딩
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# 토크나이저 사용하여 텍스트 데이터를 시퀀스로 변환
tokenizer = Tokenizer(num_words=10000)  # 빈도수가 높은 상위 10,000개의 단어만 사용
tokenizer.fit_on_texts(X)
X = tokenizer.texts_to_sequences(X)
X = pad_sequences(X, maxlen=100)  # 시퀀스 길이를 100으로 맞춤

# 데이터 분리 (훈련 데이터와 테스트 데이터)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 모델 정의
model = keras.Sequential([
    Embedding(input_dim=10000, output_dim=32, input_length=100),
    LSTM(64),
    Dense(4, activation='softmax')
])

# 모델 컴파일
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# 모델 훈련
num_epochs = 10
batch_size = 32

model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size, verbose=1)

# 모델 평가
test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)
print(f'Test accuracy: {test_accuracy * 100:.2f}%')
```

> [!NOTE] tokenizer = Tokenizer(num_words=10000)
> `tokenizer = Tokenizer(num_words=10000)` 코드에서 `num_words` 매개변수는 Tokenizer의 설정 중 하나입니다. 이 설정은 Tokenizer가 구축하는 단어 사전의 크기를 제한하는 역할을 합니다. 각 단어에 고유한 정수 인덱스가 할당되고, 이 인덱스는 각 단어의 등장 빈도에 따라 정렬됩니다.

`num_words` 매개변수를 설정하는 이유는 다음과 같습니다:

1. **단어 사전 크기 제한**: 텍스트 데이터에는 수많은 단어가 포함될 수 있습니다. 그러나 많은 경우, 모든 단어를 사용하는 것은 모델의 복잡성을 증가시키고 학습 및 예측에 불필요한 시간과 메모리를 소비할 수 있습니다. 따라서 흔히 사용되는 상위 N개의 단어만을 고려하여 모델을 훈련하고 다루는 것이 일반적입니다. `num_words`를 설정하면 Tokenizer는 등장 빈도가 가장 높은 상위 N개의 단어만을 고려합니다.

2. **빈도수가 낮은 단어 제거**: 등장 빈도가 매우 낮은 단어들은 모델에 노이즈를 추가하고, 일반화 성능을 저하시킬 수 있습니다. `num_words`를 설정하면 Tokenizer는 빈도가 낮은 단어를 무시하고 단어 사전에 포함하지 않습니다.

3. **메모리 사용량 제한**: 큰 단어 사전은 메모리 사용량을 증가시킬 수 있습니다. 특히 대규모 데이터셋을 다룰 때, 단어 사전 크기를 제한함으로써 메모리 사용량을 효율적으로 관리할 수 있습니다.

`num_words`를 설정하면 Tokenizer는 빈도가 가장 높은 N개의 단어만을 고려하여 단어 사전을 구축하고, 나머지 단어는 무시합니다. 이것은 모델의 복잡성을 줄이고 데이터를 효율적으로 처리하는 데 도움이 됩니다. `num_words`의 적절한 값은 데이터의 특성 및 작업에 따라 다를 수 있으며 실험을 통해 조정해야 합니다.

- 모델의 복잡성을 증가 시켜 모든 데이터에 반응하기 좋게 만드는 것이 좋을까

---

```python
model = keras.Sequential([

    Embedding(input_dim=10000, output_dim=32, input_length=100),

    LSTM(64),

    Dense(4, activation='softmax')

])
```

Keras를 사용하여 순환 신경망(RNN)을 기반으로한 텍스트 분류 모델을 정의하는 부분입니다. 각 레이어와 설정을 설명하겠습니다:

`input_dim=10000`: 단어 사전의 크기로, 앞서 Tokenizer를 사용하여 구축한 단어 사전의 크기를 나타냅니다. 이 값은 `num_words`로 설정되었으므로 가장 빈도가 높은 상위 N개의 단어만을 고려합니다.


---


> [!NOTE] Embedding 
>Embedding(input_dim=10000, output_dim=32, input_length=100)
>위의 코드 라인은 Keras의 Embedding 레이어를 정의하는 부분입니다.
   Embedding 레이어는 주로 텍스트 데이터나 범주형 데이터를 수치형으로 변환하여 모델이 이해할 수 있도록 하는 데 사용됩니다. 
   이 레이어는 다음과 같은 주요 매개변수로 구성됩니다:

- `input_dim`: 어휘 사전의 크기를 나타내며, 입력 데이터에 있는 고유한 단어(또는 범주)의 수입니다. 이 매개변수는 입력 데이터의 단어 인덱스가 이 범위 내에 있어야 함을 나타냅니다.

- `output_dim`: 임베딩 벡터의 차원을 지정합니다. 즉, 각 입력 단어(또는 범주)는 이 차원으로 임베딩됩니다. 이 값은 모델의 하이퍼파라미터로 조절할 수 있으며, 일반적으로 작은 차원에서 시작하여 필요에 따라 늘릴 수 있습니다.

- `input_length`: 입력 시퀀스의 길이를 나타냅니다. 이 매개변수는 주로 시퀀스 데이터의 길이를 일치시키기 위해 사용됩니다.

`Embedding` 레이어는 주로 자연어 처리(NLP) 작업에서 사용되며, 단어를 밀집된 벡터로 매핑하는 데 적합합니다. 
이렇게 벡터화된 단어는 신경망 모델의 입력으로 사용됩니다.

예를 들어, 위의 코드에서 `input_dim`이 10000으로 설정되고, `output_dim`이 32로 설정되었으므로, 어휘 사전의 크기가 10000이고 각 단어는 32차원의 임베딩 벡터로 매핑됩니다. 입력 시퀀스의 길이는 100으로 제한됩니다.

이렇게 임베딩 레이어를 사용하면 모델이 텍스트 데이터를 이해할 수 있도록 수치 데이터로 변환됩니다.


토큰나이저로 하면 안됨 단어로 구분해버리기 때문에
RNN 모델 url 전체를 봐야함
LSTM / 하이퍼볼릭탄젠트
[참조](https://velog.io/@nayeon_p00/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EB%AA%A8%EB%8D%B8-RNN-%EA%B3%BC-LSTM)
