
> [!NOTE] 미니 배치 경사 하강법
> 훈련 데이터를 작은 미니 배치로 나누어 그래디언트를 계산하고 가중치를 업데이트하는 방식입니다. 
> 일반적으로 SGD와 미니 배치 경사 하강법은 더 빠른 수렴과 메모리 효율성을 제공하며 대규모 데이터셋에서 사용됩니다.


미니 배치 확률적 경사하강법(Mini-Batch Stochastic Gradient Descent, Mini-Batch SGD)은 딥러닝 및 기계 학습에서 가중치를 업데이트하기 위한 최적화 알고리즘 중 하나입니다. 
이 알고리즘은 경사 하강법(Gradient Descent)의 변형 중 하나로, 데이터 세트를 작은 미니 배치(mini-batch)로 나누어 각 미니 배치에 대한 그래디언트(gradient)를 계산하고 가중치를 업데이트합니다.

미니 배치 확률적 경사하강법의 주요 특징과 원리는 다음과 같습니다:

1. **미니 배치**: 전체 데이터 세트를 한 번에 처리하지 않고, 작은 미니 배치로 나눕니다. 
	이 미니 배치는 데이터를 무작위로 선택하거나 셔플링할 수 있습니다.
    
2. **확률적 경사하강법**: 각 미니 배치에 대한 그래디언트를 계산하고 가중치를 업데이트합니다. 
	이것이 "확률적(stochastic)"이라고 불리는 이유는 각 미니 배치의 그래디언트 계산이 무작위성을 가지기 때문입니다.
    
3. **가중치 업데이트**: 각 미니 배치에 대한 그래디언트를 계산한 후, 이를 사용하여 가중치를 업데이트합니다. 가중치 업데이트는 경사 하강법과 유사하게 이루어집니다.


- 빠른 수렴: 전체 데이터 세트를 처리하는 것보다 빠르게 수렴할 수 있습니다.
- 메모리 효율: 전체 데이터를 메모리에 저장하지 않고도 학습할 수 있습니다.
- 무작위성: 확률적인 성격을 가지므로 지역 최소값에 빠지지 않고 더 나은 글로벌 최소값을 찾을 가능성이 높습니다.

미니 배치 확률적 경사하강법은 딥러닝에서 주로 사용되며, 데이터셋이 매우 크거나 메모리 한계로 인해 전체 데이터를 한 번에 처리하기 어려운 경우에 특히 유용합니다.
배치 크기(mini-batch size)는 하이퍼 파라미터로서 조절할 수 있으며, 일반적으로 작은 배치 크기를 선택하여 모델 학습 중에 일정한 무작위성을 유지합니다.

**적용 시점**: 경사 하강법은 일반적으로 대규모 데이터셋에 사용되며, 그래디언트를 정확하게 계산하고자 할 때 적합합니다.


